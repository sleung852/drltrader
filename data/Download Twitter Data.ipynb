{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Market News from Twitter\n",
    "So all major finance news sources have twitter accounts. In fact even one news institution may have multiple accounts. For example, Financial Times has @FT, @FinancialTimes, @FTMarkets, @fteconomics etc. <br><br>  So I decided to scrape the following market related accounts:\n",
    "1. @FTMarkets - Financial Times Markets (21.2K followers)\n",
    "2. @markets - Bloomberg Markets (987.8K followers)\n",
    "3. @WSJbusiness - Wall Street Journal Business (1.6M followers)\n",
    "4. @ReutersBiz - Reuters Business (2.2M followers)\n",
    "5. @BBCBusiness - BBC Business (1.9M followers)\n",
    "6. @CNBC - CNBC (4.1M followers)\n",
    "7. @TheEconomist - The Economist (main account) (25.7M followers) [optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from snscrape.modules import twitter\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How snscrape works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # simple test \n",
    "# keyword = 'from:@FT since:2021-04-01 until:2021-04-02'\n",
    "# scraped_tweets = twitter.TwitterSearchScraper(keyword).get_items()\n",
    "# sliced_scraped_tweets = itertools.islice(scraped_tweets, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build my own Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_acs = {\n",
    "    'ftmarkets': '@FTMarkets',\n",
    "    'bloombergmarkets': '@markets',\n",
    "    'wsjbusiness': '@WSJbusiness',\n",
    "    'reutersbusiness': '@ReutersBiz',\n",
    "    'theeconomist': '@TheEconomist',\n",
    "    'bbcbusiness': '@BBCBusiness',\n",
    "    'cnbc': '@CNBC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure data director folders exist\n",
    "data_dir = 'twitter/'\n",
    "targets = list(tar_acs.keys())\n",
    "for name in targets:\n",
    "    tar_dir = os.path.join(data_dir, name)\n",
    "    if not os.path.isdir(tar_dir):\n",
    "        os.mkdir(tar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return a range of dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "# function to scrape twitter posts in general from a specific account\n",
    "def scrape_twitter(target, start_date, end_date, limit=10000):\n",
    "    query_str = f'from:{tar_acs[target]} since:{str(start_date)} until:{str(end_date)}'\n",
    "    twt_generator = twitter.TwitterSearchScraper(query_str).get_items()\n",
    "    sliced_scraped_tweets = itertools.islice(twt_generator, limit)\n",
    "    return pd.DataFrame(sliced_scraped_tweets)\n",
    "\n",
    "# function to scrape twitter posts in batch from a specific account\n",
    "def scrape_twitter_batch_and_save(target, start_date, end_date, wait_time=2, daily_limit=10000):\n",
    "    dates = [single_date for single_date in daterange(start_date, end_date+timedelta(1))]\n",
    "    for i in trange(len(dates)-1, desc=target):\n",
    "        file_name = f'{target}_{str(dates[i])}.csv'\n",
    "        file_path = os.path.join(data_dir, target, file_name)\n",
    "        # only download tweet if not yet downloaded\n",
    "        if not os.path.isfile(file_path):\n",
    "            df = scrape_twitter(target, dates[i], dates[i+1], limit=daily_limit)\n",
    "            df.to_csv(file_path, index=None)\n",
    "            time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "# df = scrape_twitter('cnbc', date(2021, 4, 1), date(2021, 4, 2))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff69417adf34ee6a00eabb6809bccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aceadebd3d1e433da6f0abedd5c67254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ftmarkets:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f084214a158447f86b0c533bbfc4f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bloombergmarkets:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959b960dac584d4ba144aa4f8986946a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wsjbusiness:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d49f12dd5be446fa835ee1a4494ee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reutersbusiness:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e7e38618db4a6fb2fe6ea1f990abab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "theeconomist:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb2bee20e54d49942b0279c6e7f639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bbcbusiness:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa733b980b44f638b5ad818efa7223d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cnbc:   0%|          | 0/3287 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in trange(len(targets), desc='Targets'):\n",
    "    scrape_twitter_batch_and_save(targets[i], date(2010, 1, 1), date(2019, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scraper",
   "language": "python",
   "name": "env_scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
